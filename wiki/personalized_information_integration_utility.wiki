#summary One-sentence summary of this page.

= Personalized Information Integration Utility =
*by Jegan Lingam and Arvind Venkataraman*

* Abstract *

The purpose of the utility is to provide users with a tool which can retrieve information from the web based on their personal preferences, browsing history and search pattern. To ascertain the user's browsing pattern the utility monitors the user's browsing activity over a period of time. When the user is on the move and needs some information, the user needs to send a SMS or E-Mail from his mobile device to the utility. The utility checks for new mails or messages on a regular basis and based on the content of the message received, the utility checks the user's browsing history to find what he had earlier browsed that matches the information he currently needs. It then extracts information from sites for which the wrappers are present and then compares the results from various sites. The utility then provides the best available information to the user by calling the user and reading the information over the phone. In addition the system also sends the result as an E-Mail to the user.

*Introduction*

A system that keeps user abreast of latest information with minimal effort from his side especially when he does not have access to a computer or internet was the motivation for developing this utility. To begin with, the user needs to install this utility along with skype program which is needed by the utility to call up the user to read out the result. The user then registers his number with the utility. The user then gets to choose as to which sites the program should monitor and sites which the program should not monitor for user's browsing activity. The list of sites is limited by the number of wrappers available with the program. The program when enabled, records the user's keystrokes on the sites which were included in the list of the sites to be monitored and stores them for future use. The program then keeps polling a mail server for incoming mail with a particular subject. Based on the content of the message the domain of user's required information is determined. For instance if the mail reads 'get deals' then the user may need latest information on product or airline deals. We then go through the user's browsing history to see as to for which product the user had searched the most. The utility then searches the sites selected by the user to get latest deals. The utility then matches data from various sites to get a price comparison for the product that matches the search the best and then calls up the user to read out the information. This utility also address privacy concerns as this is a standalone application installed on the individual's system and the browsing pattern of the user is stored locally.


*Basic Approach*

The user activity monitoring module runs in the background once the application is started and is not visible to the user. The module checks the window caption to see if the current window is supposed to be monitored (sites to be monitored are mentioned by the user when the application is set up) and if the window needs to be monitored the application listens for users key strokes. Now when the E-Mail is received by the system it checks the database to see as to for which phrase the user has searched most. The phrases in the database are string matched using weights so as to allow for slight variations in their spellings. These phrases are then searched in the sites for which wrappers exist in the system. The wrappers are based on the page structure of the websites and we use regular expression to retrieve the product details. We then look for any particular attribute of the product which can be used to uniquely identify products across all sites. One such attribute could be the model number. The best possible result for the search should appear in the first few results of all the sites and the model number extracted from any of the site can be used to match the product across all the sites.

*Integration Approach*

We designed a mediated schema table which contains the model number, product description, cost and the source site and then we link the records using the model number obtained from the previous step. We again do not match the string exactly, but assign a score based on the number of matching characters and if the score is above a certain threshold we say that the products are same. The products that are identified to be same from various sources are linked by an attribute called linked_id. In other, words those rows with same linked_id in the database are said to be the same product but from different sources.

Apart from allowing us to achieve a good F measure using the model number to match records allows us to trim down on unnecessary results obtained from the sites. Not all sites would have same set for results excepting for the one that exactly matches the search.

*Details of the approach*

We used Visual Studio .Net framework along with host of other Win32 APIs and MS SQL Server to implement the utility. The language used was C# .Net. We also maintain three relations one for storing user‟s browsing patter, one for storing mediated schema and one for storing after performing record linkage. The application uses the OLEDB component object model (COM) provided by Microsoft to access the databases stored in the MS SQL server. The E-Mail to which the user sends the message is configured in Microsoft Outlook and the utility makes use of certain „Outlook Adapter‟ COM component to read and write E-Mails. The utility keeps polling the inbox for new messages after a fixed interval of time. Once the utility is started and the initial configuration steps are done, the utility goes into the listener mode and runs in the background. In this mode we use Win 32 API to retrieve the title of the currently active window and thus get the website name if the window is a browser window. If this is in the monitor list specified by the user, the utility then uses another Win 32 API to listen for key strokes of the user. When the user submits the request we terminate the current phrase and add it to the database used to store user‟s browsing pattern. The wrapper generation component implemented using C# attaches the search term to the URL of search page of various sites and retrieves the source code of the page. We store the wrapper structure in a XML file with specific tags like the search_start_tag, search_end_tag. The wrapper component then tries to match for the search_start_tag and then searches for a search_end_tag. At first the text between the first occurring search_start_tag and last occurring search_end_tag is chosen. We then try to iteratively do the same comparison process on the text that gets extracted. If there is a repetitive occurrence of the start tag and the end tag then this means that we are actually going through the data part of the page. This can be done using regular expression and this allows us to extract the various search results. This also enables user to input different start and end tags in the XML file and thereby allowing him to have a different wrapper model.

The wrapper module of the utility is triggered by an incoming message whose contents match a specific string used by the utility. Once the utility gets the search results from the specified sites it splits the contents to match the attributes of the mediated schema. The record linkage module then gets invoked and the operations that need to be performed by this module include blocking, field matching, record matching and entity matching. Since we only operate on the first page results of the search query we didn't find a need to perform blocking. This module tries to perform field matching on the data in the mediated schema table based on a key that is specific to the given domain, for instance model number attribute for any electronic product. We try to perform record linkage by comparing this particular key attribute. To perform string matching we perform a character wise comparison on the strings and then calculate the ratio of number of matches to the total number of comparisons and if this ratio is greater than a certain threshold we say that the strings match. The threshold is specific to each domain, for example when the domain has attribute to uniquely identify the product like the model number for electronics domain the threshold is kept high and for the domains where finding such attributes is difficult we have a lower threshold. This also helps us to remove unwanted data that do not match the search. We also output any row as result if only it is returned in at least two searches. This also helps us to remove unwanted data that do not match the search. We also output any row as result if only it is returned in at least two searches. Upon finding a match in the data stored in the mediated schema database, we assign an attribute called linked_id to the data. Set of rows are supposed to be referring to the same product or information in general if they have the same value for the linked_id attribute. Once the best results from various sites are identified and linked we use the skype API to make a call to the user‟s phone. We then link the stereo out of the computer to the input of skype call programmatically using another Win 32 API. The result in text form is converted into voice using open voice API. The result actually would be read out loud, but since the computer‟s stereo out is linked to the input of the skype call the result is read out to the user. This result is also sent as an HTML E-Mail to the user using the Microsoft Outlook adapter Component Object Model (COM). On the similar lines, when the user sends a message that reads “get tickets” the systems retrieves location information for his next appointment from his calendar and then again queries few airline ticketing sites (again those sites that have wrappers loaded in the system) and retrieves the best deal for the user. Here again we retrieve flight information as one big string from different sites and then use the flight number to match flights from different (record linkage is done here). We then call up the user using skype program and read out the result to the user. The result is also sent in an E-Mail form, so that the user can have it for future reference. We use a text to voice to convert the text into speech and then programmatically link systems stereo out to skype in. Technicalities are explained later. This can also be expanded to other domains like sports scores/news, stocks etc. We will have to write additional wrappers for these sites and may be required to use different record linkage mechanism.

Use of open speech API for converting text to speech helps in aiding clear voice communication and with addition of language support to open speech API, translation from English to other languages can be performed. This also opens up several possible application of the utility. For example, the utility can be configured to extract information from sites and communicate to users who are non English audience. Performance of open speech API is measure based on the stream quality. Currently open speech API packages comes in 8-bit and 16-bit versions, 32 bit version of SDK is currently under development. With 16 bit version of open speech we can convert text to voice and can leave a voice message for the user if he does not pick up the call for any reason. Skype API provides interface for even receiving or sending DTMF tones across the communication channel. This provides low cost communication channel for the utility otherwise we need high cost PBX systems and costly hardware setup. And moreover with visual studio for skpe API, we can design workflow effectively in such a way that the utility can be extended for multitude of applications. For example, current application can be extended to support user input and can even help the user to send personalized commands, press # to buy the ticket, press 1 for more options, etc. Visual interface for voice is given with the help of Microsoft agent control and which requires acs files which comes by default with any version of windows. The visual characters can be customized based on user preference, acs files can be found under windows MSAgent directory. Utility by default takes the default windows speech voice configured by user, which can be changed from the control panel. Emails are generated using MS outlook component object model, program sends email to ms outlook outbox and the email is sent based on the send/receive interval settings configured for the user‟s system. This can be modified to point to a different SMTP server in the config file provided in the application directory. Microsoft SMTP server which is part of internet information services can be used for this purpose. This utility after installation pops up a security trust notification from outlook application. This is to make sure the application is trusted and has methods to access all the methods of the outlook configured in the system. Google chart APIs are used to generate different types of charts are used in the data analytics section of the utility. User has the option to generate graph which can show the site which provided the most number of deals, frequently searched term in different sites that are given in the monitoring list. If the monitoring list is not specified, utility by default monitors all the sites that are accessed using any browser. The win32 API used in the utility supports any browser running in win32 API regardless of the underlying architecture. Dynamic data binding is carried out using the data binding adapter ActiveX control available as a COM component. This provides the required interface and translation between the database and user interface layer. Utility runs the website listener and the email listener module in separate threads and hence when an email comes from the user, the utility automatically switches to the integration module and starts the search term analysis and the subsequent steps of record linkage and other process.

Wrappers that are loaded during the one time initialization are used to retrieve data from several sources such as amazon.com, buy.com, circuitcity.com, bestbuy.com as raw data and stored in a mediated schema. This is used as a base for record linkage process. Labeling information collected using the wrappers are stored in a separate table and this helps to provide more information about the data present in the mediated schema. This Meta data helps to uniquely identify a label for example model number which is used as a reference to classify other data retrieved from various sites. This process is very effective if there is a unique field which can identify any record. This is one reason why product with model number or unique name gets classified accurately. After classifying into different fields, the record linker module submits a request to a comparison module which runs a series of comparison based on different rules specified. Transformation weights are calculated for the individual matching and final results are decided based on matching with highest weight for a particular linked product from different data source. For example, if the request from user is “get deals” the results of the comparison are used to check for the least price for the top 2 products user searched and this is communicated back to the user.

For this application, we have developed and loaded the following wrappers into the utility; they are amazon.com wrapper, buy.com, circuitcity.com, expedia.com, Travelocity.com, orbitz.com, priceline.com, shopping.com, ebay.com, reuters.com, nytimes.com, latimes.com. Other feeds used in the utility are RSS feeds from yahoo news and digg.com. The utility is also scalable in sense that it can call up many users at the same time to give the information. However this facility is limited by the bandwidth of the network provided.

Retrieving the meeting information from google calendar is carried out by using the google APIs which provides interface and authentication mechanism to connect to user‟s private calendar. The same set of processes happen for retrieving flight deals, but we retrieve the destination of the flight from the information stored in the user‟s Google calendar. The destination is the location of the next event in the user‟s calendar. We use the reference set approach to find out the location. We have a possible list of cities, states and their acronyms stored. We then try to compare the Location component of the Google calendar entry with those present in this list and try to obtain a match. We again generate wrappers for the flight deals site, get the mediated schema and link records to match same flights across different sites. In both the product deals and airline deals the row with the lowest price is what is returned to the user in terms of voice and an E-Mail. We also provide the user with an option of viewing the analytics of the data collected by the utility. The data that is shown could be either the most frequently searched terms, or the sites that provide the best possible results. The analytics are shown using various different chart forms like pie chart, bar chart, 3D analysis to name a few. We display these charts using Google chart API available online. We pass the data generated as input to the Google chart API and this then returns the URL of the image of the chart generated. The image is then retrieved and displayed to the user.

*Empirical Evaluation*

We use the F-measure parameter to evaluate the accuracy of the record linkage output generated by the utility. We check to see whether the records that were identified by the systems to be same, were actually representing the same entity. For instance a set of records having the same model number for an electronic product represent the same product and we check to see whether the utility identifies them as representing the same product. We compute the F-measure as the harmonic mean of Precision (P) and Recall(R) i.e. F = 2PR/(P + R) Precision is defined as the ratio of number of correctly predicted matches to the total number of predicted matches, i.e. P = # correctly predicted matches/ # total predicted matches Recall is defined as the ratio of number of correctly predicted matches to the actual count of matches present in the data, i.e. R = # correctly predicted matches/ # actual number of matches present We evaluated the performance of our system based on the record linkage process for various domains. We tried to identify as to how many products from various sites were matched correctly. We plotted the F-measure value for these domains and obtained the following graph:

Upon evaluation we find that our utility achieves a very high F-value in Technology (electronics) and air tickets domains. This could be attributed to the fact that these domains have a certain attribute which can be used to identify products or flights. For instance a model number is unique for any given electronic product and flight number for the airline ticketing domain to identify flights uniquely. The system performance goes down relatively in the Software domain and this can attributed to the fact that acronyms and short forms used to identify software leads to slightly lower F-value. The utility performs the worst in the apparel domain. This can be attributed to lack of any particular that identifies apparel uniquely. And the fact that there can be many shapes, sizes and color for apparel didn‟t help matters either.

*Addressing Privacy Concerns* 

User‟s browsing history and other details captured are stored locally instead of in a third party system, this helps to keep the data private. Utility runs on the local pc and can be configured to point to any Microsoft SQL server instance hosted with a help of the initialization module. This addresses the data privacy concerns of information sharing prevailing in the industry currently.

*Future Work*

Automatic wrapper learning techniques can be incorporated into the utility so as to allow dynamic generation of wrappers. This enables the utility to work on more number of web pages than it is able to do at the moment. This also enables the system to handle sources that are bound to be changing over a period of time. We can also try to include interactive voice response into the system so that it allows for two-way interaction between the user and the system. This might be useful in situations like where the utility reads out a airline ticket deal and ask for the user to press a key on his device to book the ticket online.

*Conclusion*

In this paper we have presented a utility which enables the user to extract information based on user‟s personal preferences. This utility is designed using several information integration techniques to retrieve unstructured data and structure them based on several parameters. Utility along with the flexible anywhere communication model can be very efficient in information retrieval domain. This reduces the need for a full fledged browser to get the required information. We have also evaluated our system based on correctness of the information extracted.

*References*

1) Lecture on Record Linkage by Craig A. Knoblock for Information Integration on Web course (Spring 2008)

2) Lecture on Blocking Schemes for Record Linkage by Matthew Michelson for Information Integration on Web course (Spring 2008)

3) Lecture on Automatic Wrapper Generation by Kristina Lerman for Information Integration on Web course (Spring 2008)

4) Kristina Lerman, Lise Getoor, Steven Minton and Craig Knoblock. Using the Structure of Web Sites for Automatic Segmentation of Tables. In SIGMOD 2004 June 13-18, 2004, Paris, France.

5) W. Crescenzi, G. Mecca, P. Merialdo. RoadRunner: Towards Automatic Data Extraction from Large Web Sites. In The VLDB Journal, 109-118, 2001.

6) B. Cenk Gazen and Steven Minton. Overview of AutoFeed: An Unsupervised Learning System for Generating Webfeeds. Proceedings of AAAI, 2006

7) Matthew Michelson and Craig A. Knoblock. Learning Blocking Schemes for Record Linkage.

8) In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-2006), Boston, MA, 2006.

9) Mikhail Bilenko and Raymond J. Mooney. Adaptive Duplicate Detection Using Learnable String Similarity Measures. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2003), pp.39-48, Washington, DC, August 2003.